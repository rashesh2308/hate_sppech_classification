{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GAN_Based_DocVec_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvhW_jk62gar"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPoRrYnZ6NCy"
      },
      "source": [
        "d = pd.read_csv(\"labelled_dataset_temp.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puvZdQCO6cx-"
      },
      "source": [
        "tweetlist = d['text'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir4fnZ4w3tdt"
      },
      "source": [
        "import re\n",
        "contractions = { \n",
        " \"ain't\":\"am not\",\"aren't\":\"are not\",\"can't\":\"cannot\",\"can't've\":\"cannot have\",\"'cause\":\"because\",\"could've\":\"could have\",\"couldn't\":\"could not\",\"couldn't've\":\"could not have\",\"didn't\":\"did not\",\"doesn't\":\"does not\",\"don't\":\"do not\",\"hadn't\":\"had not\",\"hadn't've\":\"had not have\",\"hasn't\":\"has not\",\"haven't\":\"have not\",\"he'd\":\"he had\",\"he'd've\":\"he would have\",\"he'll\":\"he will\",\"he'll've\":\"he will have\",\"he's\":\"he has\",\"how'd\":\"how did\",\"how'd'y\":\"how do you\",\"how'll\":\"how will\",\"how's\":\"how is\",\"I'd\":\"I would\",\"I'd've\":\"I would have\",\"I'll\":\"I will\",\"I'll've\":\"I will have\",\"I'm\":\"I am\",\"I've\":\"I have\",\"isn't\":\"is not\",\"it'd\":\"it would\",\"it'd've\":\"it would have\",\"it'll\":\"it will\",\"it'll've\":\"it will have\",\"it's\":\"it is\",\"let's\":\"let us\",\"ma'am\":\"madam\",\"mayn't\":\"may not\",\"might've\":\"might have\",\"mightn't\":\"might not\",\"mightn't've\":\"might not have\",\"must've\":\"must have\",\"mustn't\":\"must not\",\"mustn't've\":\"must not have\",\"needn't\":\"need not\",\"needn't've\":\"need not have\",\"o'clock\":\"of the clock\",\"oughtn't\":\"ought not\",\"oughtn't've\":\"ought not have\",\"shan't\":\"shall not\",\"sha'n't\":\"shall not\",\"shan't've\":\"shall not have\",\"she'd\":\"she would\",\"she'd've\":\"she would have\",\"she'll\":\"she will\",\"she'll've\":\"he will have\",\"she's\":\"she is\",\"should've\":\"should have\",\"shouldn't\":\"should not\",\"shouldn't've\":\"should not have\",\"so've\":\"so have\",\"so's\":\"so is\",\"that'd\":\"that would\",\"that'd've\":\"that would have\",\"that's\":\"that is\",\"there'd\":\"there would\",\"there'd've\":\"there would have\",\"there's\":\"there is\",\"they'd\":\"tthey would\",\"they'd've\":\"they would have\",\"they'll\":\"they will\",\"they'll've\":\"they will have\",\"they're\":\"they are\",\"they've\":\"they have\",\"to've\":\"to have\",\"wasn't\":\"was not\",\"we'd\":\"we would\",\"we'd've\":\"we would have\",\"we'll\":\"we will\",\"we'll've\":\"we will have\",\"we're\":\"we are\",\"we've\":\"we have\",\"weren't\":\"were not\",\"what'll\":\"what will\",\"what'll've\":\"what will have\",\"what're\":\"what are\",\"what's\":\"what is\",\"what've\":\"what have\",\"when's\":\"when is\",\"when've\":\"when have\",\"where'd\":\"where did\",\"where's\":\"where is\",\"where've\":\"where have\",\"who'll\":\"who will\",\"who'll've\":\"who will have\",\"who's\":\"who is\",\"who've\":\"who have\",\"why's\":\"why is\",\"why've\":\"why have\",\"will've\":\"will have\",\"won't\":\"will not\",\"won't've\":\"will not have\",\"would've\":\"would have\",\"wouldn't\":\"would not\",\"wouldn't've\":\"would not have\",\"y'all\":\"you all\",\"y'all'd\":\"you all would\",\"y'all'd've\":\"you all would have\",\"y'all're\":\"you all are\",\"y'all've\":\"you all have\",\"you'd\":\"you would\",\"you'd've\":\"you would have\",\"you'll\":\"you will\",\"you'll've\":\"you will have\",\"you're\":\"you are\",\"you've\":\"you have\"\n",
        "}\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
        "\n",
        "def expand_contractions(s, contractions_dict=contractions):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVdjcpcQx8nf"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "import re\n",
        "def clean(tweet):\n",
        "    # remove hashtag\n",
        "    tweet = re.sub(r'@\\w+:?',' ',tweet)\n",
        "    # remove retweet symbol\n",
        "    tweet = re.sub(r'^RT',' ',tweet)\n",
        "    # remove hashtags\n",
        "    tweet = re.sub(r'#\\w+',' ',tweet)\n",
        "    # remove URL\n",
        "    tweet = re.sub(r'https\\S+',' ',tweet)\n",
        "    # replace ’ with '\n",
        "    tweet = re.sub(r'’',\"'\",tweet)\n",
        "    # expand contraction\n",
        "    tweet = expand_contractions(tweet)\n",
        "    # remove non-word character\n",
        "    tweet = re.sub(r'[^\\w\\s]',' ',tweet)\n",
        "    # remove extra space\n",
        "    tweet = re.sub(r'\\s+',' ',tweet)\n",
        "    # remove amp\n",
        "    tweet = re.sub(r'\\samp\\s',' ',tweet)\n",
        "    # convert to lower case and strip leading and trailing spaces\n",
        "    tweet = tweet.lower().strip()\n",
        "    # tokenizing\n",
        "    words = tokenizer.tokenize(tweet)\n",
        "    # remove words having numbers\n",
        "    words = [w for w in words if re.search(r'[0-9]',w) == None]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRPaUqWdCM4K"
      },
      "source": [
        "# pasting the cleaned_tweetlist back in the dataframe\n",
        "cleaned_tweetlist = list(map(lambda x: clean(x), tweetlist))\n",
        "d['cleaned']=cleaned_tweetlist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ7NZPEtE0hq"
      },
      "source": [
        "# getting rid of very small length tweets\n",
        "d = d[d['cleaned'].apply(lambda x:len(x)>2)]\n",
        "cleaned_tweetlist = d['cleaned'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDUczf6Y_xr"
      },
      "source": [
        "# from itertools import compress\n",
        "# mask = list(map(lambda x: len(x)>10, cleaned_tweetlist))\n",
        "# cleaned_tweetlist = list(compress(cleaned_tweetlist,mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az9wn1ZaLhAR"
      },
      "source": [
        "# create document vectors using gensim doc2vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(cleaned_tweetlist)]\n",
        "model_d2v = Doc2Vec(vector_size=32, window=3, min_count=4, workers=4, negative = 10, epochs=40)\n",
        "model_d2v.build_vocab(documents)\n",
        "model_d2v.train(documents, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU5eYgzHSeJJ"
      },
      "source": [
        "document_vectors_all = list(map(lambda p:model_d2v.infer_vector(p),cleaned_tweetlist))\n",
        "d['docvec'] = document_vectors_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq6dS2RztD-I"
      },
      "source": [
        "From here we will split 20% of the data and use it for test, since these are the labelled data.\n",
        "\n",
        "With rest of the data, the hatespeeches among them will be fed to GAN to generate more document vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt7T8lOLtBbA"
      },
      "source": [
        "test = d.sample(frac=0.2)\n",
        "rest = pd.concat([d,test]).drop_duplicates(subset=[\"tweet_id\"],keep=False)\n",
        "document_vectors = rest[rest['hate'].apply(lambda x:x==1.0)].docvec.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znhqkr7wDga7"
      },
      "source": [
        "<h1>Generation of Document Vectors using GAN</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puBigANZS9x9"
      },
      "source": [
        "# Generation of Document Vectors using GAN\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "train_documents = np.array(document_vectors)\n",
        "train_documents = train_documents.reshape(train_documents.shape[0],32,1)\n",
        "# Elements are already normalized to [-1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ-HcTq0Yo0E"
      },
      "source": [
        "s_buffer = 1000\n",
        "s_batch = 256\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_documents).shuffle(s_buffer).batch(s_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbUCcRH9ZkeI"
      },
      "source": [
        "# Now let us define the generator\n",
        "# Generator will accept (100,) noise and will generate a document vector of length 32\n",
        "from tensorflow.keras import layers\n",
        "def gen_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(8*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((8,256)))\n",
        "    assert model.output_shape == (None, 8, 256)\n",
        "\n",
        "    model.add(layers.Conv1DTranspose(filters=128, kernel_size=5, strides=1, padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 8, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv1DTranspose(filters=64, kernel_size=5, strides=2, padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 16, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # tanh activation as output needs to be in range [-1,1]\n",
        "    model.add(layers.Conv1DTranspose(filters=1, kernel_size=5, strides=2, padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 32, 1)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajwtMTxLDJ33",
        "outputId": "4bd1a8c0-76fb-4b9f-d21f-ce5bf41fabb5"
      },
      "source": [
        "generator = gen_model()\n",
        "random_noise = tf.random.normal([1, 100])\n",
        "generated_document = generator(random_noise, training=False)\n",
        "generated_document[0, :, 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
              "array([-0.06277785, -0.03147944, -0.1161528 ,  0.04554995,  0.02325949,\n",
              "       -0.12488685, -0.09861955, -0.06390463, -0.12079295, -0.08284715,\n",
              "       -0.04897938, -0.10832902,  0.0880328 , -0.02016436, -0.23562035,\n",
              "       -0.01801194,  0.09127183,  0.18714127,  0.03508518, -0.01288858,\n",
              "        0.03300442,  0.02713495, -0.04917263, -0.12127037,  0.07037178,\n",
              "        0.12309948, -0.04171433, -0.02358885,  0.13083865,  0.11986753,\n",
              "        0.02226896,  0.06582754], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d00lVtOMbceV"
      },
      "source": [
        "# we will create a discriminator here\n",
        "# the discriminator is a document classifier\n",
        "def disc_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=5, strides=2, padding='same', input_shape=[32, 1]))\n",
        "\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv1D(filters=128, kernel_size=5, strides=2, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVQMnXhbFj0K",
        "outputId": "6972c944-fbab-4ffe-cb7c-43816ee2d98e"
      },
      "source": [
        "discriminator = disc_model()\n",
        "decision = discriminator(generated_document)\n",
        "print(decision)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.00196937]], shape=(1, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ3mW2kHFzoG"
      },
      "source": [
        "# we will use cross_entropy loss\n",
        "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieRJDY5MGK6I"
      },
      "source": [
        "# discriminator loss\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy_loss(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy_loss(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "# generator loss\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy_loss(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pz5VQB9GY54"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne_MO0HJG9nA"
      },
      "source": [
        "epochs = 50\n",
        "noise_dim = 100\n",
        "n_generate = 4000\n",
        "seed = tf.random.normal([n_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6-OzDuCHlqc"
      },
      "source": [
        "@tf.function\n",
        "def training_step(documents):\n",
        "    noise = tf.random.normal([s_batch, noise_dim])\n",
        "    with tf.GradientTape() as gtape, tf.GradientTape() as dtape:\n",
        "        generated_documents = generator(noise, training=True)\n",
        "        real_output = discriminator(documents, training=True)\n",
        "        fake_output = discriminator(generated_documents, training=True)\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "    grad_gen = gtape.gradient(gen_loss, generator.trainable_variables)\n",
        "    grad_disc = dtape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(grad_gen, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(grad_disc, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8f-1k94KxFg"
      },
      "source": [
        "def generate_docvecs(model, epoch, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAVv82S7L2XE"
      },
      "source": [
        "# This is the training loop\n",
        "import time\n",
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    for document_batch in dataset:\n",
        "      training_step(document_batch)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  return generate_docvecs(generator, epochs, seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4xT80rVZNVO",
        "outputId": "1761bcb3-1986-49ff-b28a-9c341492577b"
      },
      "source": [
        "generated_document_vectors = train(train_dataset, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 1.8609263896942139 sec\n",
            "Time for epoch 2 is 0.010711193084716797 sec\n",
            "Time for epoch 3 is 0.010044336318969727 sec\n",
            "Time for epoch 4 is 0.009728670120239258 sec\n",
            "Time for epoch 5 is 0.01110076904296875 sec\n",
            "Time for epoch 6 is 0.011414527893066406 sec\n",
            "Time for epoch 7 is 0.010553836822509766 sec\n",
            "Time for epoch 8 is 0.012979745864868164 sec\n",
            "Time for epoch 9 is 0.009850740432739258 sec\n",
            "Time for epoch 10 is 0.009933233261108398 sec\n",
            "Time for epoch 11 is 0.00986170768737793 sec\n",
            "Time for epoch 12 is 0.010763168334960938 sec\n",
            "Time for epoch 13 is 0.009564399719238281 sec\n",
            "Time for epoch 14 is 0.009765386581420898 sec\n",
            "Time for epoch 15 is 0.009802579879760742 sec\n",
            "Time for epoch 16 is 0.00973057746887207 sec\n",
            "Time for epoch 17 is 0.009823083877563477 sec\n",
            "Time for epoch 18 is 0.009493589401245117 sec\n",
            "Time for epoch 19 is 0.010235786437988281 sec\n",
            "Time for epoch 20 is 0.011646270751953125 sec\n",
            "Time for epoch 21 is 0.014807939529418945 sec\n",
            "Time for epoch 22 is 0.013098478317260742 sec\n",
            "Time for epoch 23 is 0.01080632209777832 sec\n",
            "Time for epoch 24 is 0.010231256484985352 sec\n",
            "Time for epoch 25 is 0.0103607177734375 sec\n",
            "Time for epoch 26 is 0.010221242904663086 sec\n",
            "Time for epoch 27 is 0.00973963737487793 sec\n",
            "Time for epoch 28 is 0.009919881820678711 sec\n",
            "Time for epoch 29 is 0.010049819946289062 sec\n",
            "Time for epoch 30 is 0.009684324264526367 sec\n",
            "Time for epoch 31 is 0.009432554244995117 sec\n",
            "Time for epoch 32 is 0.008800983428955078 sec\n",
            "Time for epoch 33 is 0.009132623672485352 sec\n",
            "Time for epoch 34 is 0.010773420333862305 sec\n",
            "Time for epoch 35 is 0.011941671371459961 sec\n",
            "Time for epoch 36 is 0.011581659317016602 sec\n",
            "Time for epoch 37 is 0.009840726852416992 sec\n",
            "Time for epoch 38 is 0.012753486633300781 sec\n",
            "Time for epoch 39 is 0.009338617324829102 sec\n",
            "Time for epoch 40 is 0.009669303894042969 sec\n",
            "Time for epoch 41 is 0.013442754745483398 sec\n",
            "Time for epoch 42 is 0.009834766387939453 sec\n",
            "Time for epoch 43 is 0.009897470474243164 sec\n",
            "Time for epoch 44 is 0.009756803512573242 sec\n",
            "Time for epoch 45 is 0.009301424026489258 sec\n",
            "Time for epoch 46 is 0.009243249893188477 sec\n",
            "Time for epoch 47 is 0.010120868682861328 sec\n",
            "Time for epoch 48 is 0.009482622146606445 sec\n",
            "Time for epoch 49 is 0.009290933609008789 sec\n",
            "Time for epoch 50 is 0.010776281356811523 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF8OAr9pdbt_"
      },
      "source": [
        "# these are generated document vectors\n",
        "generated_document_vectors = np.reshape(generated_document_vectors.numpy(),(n_generate,32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIGPwDPBd6pG"
      },
      "source": [
        "data = list(zip(generated_document_vectors,np.full(n_generate, 1.0)))\n",
        "generated = pd.DataFrame(data, columns =['docvec', 'hate'])\n",
        "original = rest[['docvec','hate']]\n",
        "training_dataset = pd.concat([original,generated])\n",
        "test_dataset = test[['docvec','hate']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMoiGEwVfuoV"
      },
      "source": [
        "<h1>Building a Classifier</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lPM0tvljoNj"
      },
      "source": [
        "x_train = training_dataset['docvec'].to_list()\n",
        "x_train = np.reshape(x_train,(len(x_train),1,32))\n",
        "x_test = test_dataset['docvec'].to_list()\n",
        "x_test = np.reshape(x_test,(len(x_test),1,32))\n",
        "y_train = np.array(training_dataset['hate'].to_list(),dtype=\"uint8\")\n",
        "y_test = np.array(test_dataset['hate'].to_list(),dtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUCZgR-Id5Oi",
        "outputId": "aae7ead8-3447-4569-d0d9-2c8bc9102f54"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(392,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYphWjL51Xzk"
      },
      "source": [
        "Now let us define the classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C33PbxgiQ-U"
      },
      "source": [
        "def classification_model():\n",
        "      model = tf.keras.Sequential()\n",
        "      model.add(layers.Dense(units = 64,input_shape=(1,32),activation='relu'))\n",
        "      model.add(layers.Dense(units=96, activation='relu'))\n",
        "      model.add(layers.Dense(units=48, activation='relu'))\n",
        "      model.add(layers.Dense(units=24, activation='relu'))\n",
        "      model.add(layers.Dense(units=12, activation='relu'))\n",
        "      model.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "      return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOK9vR0C6XUb",
        "outputId": "87eaf680-7e6f-4787-ab97-765a51d7427b"
      },
      "source": [
        "c_model = classification_model()\n",
        "c_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1, 64)             2112      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1, 96)             6240      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1, 48)             4656      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1, 24)             1176      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1, 12)             300       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1, 1)              13        \n",
            "=================================================================\n",
            "Total params: 14,497\n",
            "Trainable params: 14,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq1d10WOMlGD"
      },
      "source": [
        "Defining a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybn-57LZhiFL",
        "outputId": "4ae06cc3-9e4c-4530-9e2c-75ba976076ed"
      },
      "source": [
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "c_model.compile(optimizer='adam',loss=loss_fn, metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
        "c_model.fit(x_train, y_train, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "174/174 [==============================] - 2s 5ms/step - loss: 0.3155 - accuracy: 0.8726 - precision: 0.8826 - recall: 0.9695\n",
            "Epoch 2/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.1119 - accuracy: 0.9615 - precision: 0.9996 - recall: 0.9499\n",
            "Epoch 3/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.1078 - accuracy: 0.9623 - precision: 0.9994 - recall: 0.9510\n",
            "Epoch 4/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.1101 - accuracy: 0.9573 - precision: 0.9995 - recall: 0.9445\n",
            "Epoch 5/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.1011 - accuracy: 0.9631 - precision: 0.9997 - recall: 0.9512\n",
            "Epoch 6/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0976 - accuracy: 0.9631 - precision: 1.0000 - recall: 0.9515\n",
            "Epoch 7/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0979 - accuracy: 0.9618 - precision: 0.9994 - recall: 0.9496\n",
            "Epoch 8/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0856 - accuracy: 0.9691 - precision: 0.9976 - recall: 0.9611\n",
            "Epoch 9/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0778 - accuracy: 0.9677 - precision: 0.9969 - recall: 0.9610\n",
            "Epoch 10/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0768 - accuracy: 0.9688 - precision: 0.9961 - recall: 0.9629\n",
            "Epoch 11/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0631 - accuracy: 0.9749 - precision: 0.9956 - recall: 0.9715\n",
            "Epoch 12/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0593 - accuracy: 0.9758 - precision: 0.9943 - recall: 0.9736\n",
            "Epoch 13/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0536 - accuracy: 0.9782 - precision: 0.9924 - recall: 0.9786\n",
            "Epoch 14/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0537 - accuracy: 0.9760 - precision: 0.9894 - recall: 0.9780\n",
            "Epoch 15/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0514 - accuracy: 0.9784 - precision: 0.9914 - recall: 0.9800\n",
            "Epoch 16/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0423 - accuracy: 0.9801 - precision: 0.9893 - recall: 0.9841\n",
            "Epoch 17/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0420 - accuracy: 0.9840 - precision: 0.9931 - recall: 0.9857\n",
            "Epoch 18/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0392 - accuracy: 0.9828 - precision: 0.9895 - recall: 0.9878\n",
            "Epoch 19/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0311 - accuracy: 0.9874 - precision: 0.9909 - recall: 0.9925\n",
            "Epoch 20/20\n",
            "174/174 [==============================] - 1s 5ms/step - loss: 0.0305 - accuracy: 0.9854 - precision: 0.9941 - recall: 0.9866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdd39a0d5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egQc5LpQS3ll"
      },
      "source": [
        "y_pre = c_model.predict(x_test)[:,0,0]\n",
        "y_pre = (y_pre>0.5).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPZva0-ckpJ",
        "outputId": "8358b4d9-6114-444c-acdd-89bd224ec1f0"
      },
      "source": [
        "tf.math.confusion_matrix(y_test,y_pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[309,  39],\n",
              "       [ 29,  15]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jFaIhgLRf7i",
        "outputId": "eb0c4d9f-04d9-4d85-a585-e12e2a94c318"
      },
      "source": [
        "c_model.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6474 - accuracy: 0.8265 - precision: 0.2778 - recall: 0.3409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6473866105079651,\n",
              " 0.8265306353569031,\n",
              " 0.2777777910232544,\n",
              " 0.34090909361839294]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}